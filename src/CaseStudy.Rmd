---
title: "Mapping the area of applicability - Case Study"
subtitle: "Supplementary to the paper 'Predicting into unknown space? Estimating the area of applicability of spatial prediction models'"
author: "Hanna Meyer"
date: "1/4/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
This script contains the complete code for the case study within the paper and creates the figures presented there.
It can further be used to run experiments under different settings. 
Note that running the code takes a few minutes (depending on the number of training data and the size of the study area)!

## Getting started 
Major functionality needed is the 'aoa' function from the 'CAST' package that is doing the distance based estimation of the area of applicability. 'Caret' is needed for model training. The case study uses a simulated prediction task based on the 'virtualspecies' package.

```{r libraries, message = FALSE}
rm(list=ls())
#install_github("HannaMeyer/CAST")
library(virtualspecies)
library(caret)
library(CAST)
library(viridis)
library(gridExtra)
library(knitr)
library(grid)
library(latticeExtra)
library(hydroGOF)
```

Settings for generating the predictors and response need to be defined, as well as the number of training data points and the seed used for all functions that involve randomness.
The settings specified here are used for the case study published in the paper. Feel free to change them to see how things work under different scenarios!

### Getting started 


Note: To reproduce results used in the paper use the following settings:

* Default Case study with random data: npoints=50,design="random",meansPCA= c(3, -1), sdPCA=c(2, 2),simulateResponse=c("bio2","bio5","bio10", "bio13","bio14","bio19"),studyarea=c(-15, 65, 30, 75), seed=10

* Case Study with clustered data: npoints=500,nclusters=50,design="clustered",
maxdist <- 0.6, meansPCA= c(3, -1), sdPCA=c(2, 2), simulateResponse=c("bio2","bio5","bio10", "bio13","bio14", "bio19"), studyarea=c(-15, 65, 30, 75), seed=10


Further interesting settings:

* Case Study biased with outlier: npoints=50,design="biasedWithOutlier",countries = c("Germany","Ireland","France", "Sweden"), countriesOutlier= "Turkmenistan"
maxdist <- 0.6, meansPCA= c(3, -1), sdPCA=c(2, 2), simulateResponse=c("bio2","bio5","bio10", "bio13","bio14", "bio19"), studyarea=c(-15, 65, 30, 75), seed=10

* Case Study biased: same as biased with outlier but without the outlier


```{r settings}
npoints <- 50 # number of training samples

design <- "random"  # either clustered,biased,random, biasedWithOutlier

countries <- c("Germany","Ireland","France", "Sweden") #if design==biased
countriesOutlier <- "Turkmenistan" #if design==biasedWithOutlier A single point is set here
nclusters <- 50 #number of clusters if design==clustered
maxdist <- 0.6 #maxdist for clustered samples if design==clustered
meansPCA <- c(3, -1) # means of the gaussian response functions to the 2 axes
sdPCA <- c(2, 2) # sd's of the gaussian response functions to the 2 axes
simulateResponse <- c("bio2","bio5","bio10", "bio13",
                      "bio14","bio19") # variables used to simulate the response
studyarea <- c(-15, 65, 30, 75) # extent of study area. Default: Europe
seed <- 10
multiCV <- TRUE
```

# Get data
Bioclim data are downloaded and cropped to the study area.

```{r data, message = FALSE}
predictors_global <- getData('worldclim', var='bio', res=10, path='../data/')
wp <- extent(studyarea)
predictors <- crop(predictors_global,wp)

#create a mask for land area:
mask <- predictors[[1]]
values(mask)[!is.na(values(mask))] <- 1

```


## Generate Predictors and Response
The virtual response variable is created based on the PCA of a subset of bioclim predictors. See the virtualspecies package for further information.

```{r variables,message = FALSE}
response_vs <- generateSpFromPCA(predictors[[simulateResponse]],
                              means = meansPCA,sds = sdPCA, plot=F)

response <- response_vs$suitab.raster



```

## Simulate training points
To simulate field locations that are typically used as training data, "npoints" locations are randomly selected.
If a clustered design is used, the "npoints" are distributed over "nclusters" with a maximum distance between each point of a cluster (maxdist, in degrees).


```{r clusteredpoints,message = FALSE, include=FALSE}
#For a clustered sesign:
csample <- function(x,n,nclusters,maxdist,seed){
  set.seed(seed)
  cpoints <- sp::spsample(x, n = nclusters, type="random")
  result <- cpoints
  result$clstrID <- 1:length(cpoints)
  for (i in 1:length(cpoints)){
    ext <- rgeos::gBuffer(cpoints[i,], width = maxdist)
    newsamples <- sp::spsample(ext, n = (n-nclusters)/nclusters, 
                               type="random")
    newsamples$clstrID <- rep(i,length(newsamples))
    result <- rbind(result,newsamples)
    
  }
  result$ID <- 1:nrow(result)
  return(result)
}
```

```{r samplepoints, message = FALSE, warning=FALSE}
mask <- rasterToPolygons(mask,dissolve=TRUE)
set.seed(seed)
if (design=="clustered"){
  samplepoints <- csample(mask,npoints,nclusters,maxdist=maxdist,seed=seed)
} 
if (design=="biased"){
  countryboundaries <- getData("countries", path='../data/')
  countryboundaries <- countryboundaries[countryboundaries$NAME_ENGLISH%in%c(countries),]
  samplepoints <- spsample(countryboundaries,npoints,"random")
}
if (design=="biasedWithOutlier"){
  countryboundaries <- getData("countries", path='../data/')
  countryboundariesOut <- countryboundaries[countryboundaries$NAME_ENGLISH%in%c(countriesOutlier),]
    countryboundaries <- countryboundaries[countryboundaries$NAME_ENGLISH%in%c(countries),]
  samplepoints <- spsample(countryboundaries,npoints,"random")
  samplepoints <- rbind(samplepoints,spsample(countryboundariesOut,1,"random"))
}  
  
if (design=="random"){
  samplepoints <- spsample(mask,npoints,"random")
}


```


```{r vis_data, messages = FALSE, echo = FALSE}

png("../figures/trainingdata.png", width=26, height=10,units = "cm",res=300)
p1 <- spplot(stretch(predictors[[simulateResponse]],0,1),col.regions=viridis(100),
             par.settings =list(strip.background=list(col="grey")))

p2 <-spplot(response,col.regions=viridis(100),
            sp.layout=list("sp.points", samplepoints, col = "red", first = FALSE))
grid.arrange(p1,p2,ncol=2,nrow=1,widths=c(1.25,1))

grid.text("a",x = unit(0.02, "npc"), 
          y = unit(0.96, "npc"),
          just = "left")
grid.text("b",x = unit(0.57, "npc"), 
          y = unit(0.96, "npc"),
          just = "left")


invisible(dev.off())




```

```{r figs1, echo=FALSE,out.width="100%",fig.cap="Subset of the predictors as well as the response variable and the selected training data points"}
include_graphics("../figures/trainingdata.png")
```


# Model training and prediction

## Preparation
To prepare model training, predictor variables are extracted for the location of the selected sample data locations.

```{r traindat}
trainDat <- extract(predictors,samplepoints,df=TRUE)
trainDat$response <- extract (response,samplepoints)

if (design=="clustered"){
  trainDat <- merge(trainDat,samplepoints,by.x="ID",by.y="ID")
}

trainDat <- trainDat[complete.cases(trainDat),]
```

```{r plottrdat}

#pca_pred <- predict(response_vs$details$pca,trainDat[,response_vs$details$variables])

#axes <- response_vs$details$axes
#axes.to.plot <- axes[1:2]
# xmin <- min(response_vs$details$pca$li[, axes.to.plot[1]]) - 0.3 * diff(range(response_vs$details$pca$li[, axes.to.plot[1]]))
# xmax <- max(response_vs$details$pca$li[, axes.to.plot[1]])
# 
# pdf("../figures/plotresponse.pdf",width=6,height=6)
# plotResponse(response_vs,no.plot.reset=T)
# points(((pca_pred[,1]+10)/21.5),
#        pca_pred[,2],pch=16)
# dev.off()
```

```{r figs_plotres, echo=FALSE,out.width="100%",fig.cap="Subset of the predictors as well as the response variable and the selected training data points"}
#include_graphics("../figures/plotresponse.pdf")
```


## Training and cross-validation
Model training is then done using the caret package. Note that other packages work as well as long as variable importance can be derived. The model output gives information on the general estimated model performance based on random cross validation.

```{r training}
set.seed(seed)
if(design!="clustered"){
model <- train(trainDat[,names(predictors)],
               trainDat$response,
               method="rf",
               importance=TRUE,
               tuneGrid = expand.grid(mtry = c(2:length(names(predictors)))),
               trControl = trainControl(method="cv",savePredictions = TRUE))


print("random cross-validation performance:")
print(model)
}
#if data are clustered, clustered CV is used:
if(design=="clustered"){
  folds <- CreateSpacetimeFolds(trainDat, spacevar="clstrID",k=nclusters)
  model <- train(trainDat[,names(predictors)],
                 trainDat$response,
                     method="rf",
                 importance=TRUE,
                 tuneGrid = expand.grid(mtry = c(2:length(names(predictors)))),
                 trControl = trainControl(method="cv",index=folds$index,savePredictions = TRUE))
  print("leave-cluster-out cross-validation performance:")
  print(model)
  
  model_random <- train(trainDat[,names(predictors)],
                        trainDat$response,
               method="rf",
               importance=TRUE,
               tuneGrid = expand.grid(mtry = c(2:length(names(predictors)))),
               trControl = trainControl(method="cv",savePredictions = TRUE))
  print("random cross-validation performance:")
print(model_random)
}
```


## Prediction and error calculation

The trained model is used to make predictions for the entire study area. The absolute error between prediction and reference is calculated for later comparison with the dissimilarity index.

```{r predict}
prediction <- predict(predictors,model)
truediff <- abs(prediction-response)
```

# Estimating the area of applicability

## Variable importance to get weights
The variable importance from model training can be visualized to get information on how variable weighting will be done during to estimation of the dissimilarity index

```{r varimp,message=FALSE, echo=FALSE}
cairo_pdf("../figures/varimp.pdf", width=4, height=3.5)
plot(varImp(model,scale = F),col="black")
invisible(dev.off())
```


```{r figs3, echo=FALSE, out.width="75%",fig.cap="Variable importance"}
include_graphics("../figures/varimp.pdf")
```

The area of applicability and the dissimilarity index are then calculated. First using weighted variables, and second (for comparison) without weighting. Everything is run in parallel to speed things up.

```{r uncert,message=FALSE}

#with variable weighting:
AOA <- aoa(predictors,model=model,returnTrainDI = TRUE)
#without weighting:
AOA_noWeights <- aoa(predictors,train=trainDat,variables = names(predictors))

if (design=="clustered"){
  AOA_random<- aoa(predictors, model_random)
}

preds <- model$pred[model$pred$mtry==model$bestTune$mtry,]
absError <- abs(preds$pred[order(preds$rowIndex)]-preds$obs[order(preds$rowIndex)])


predtrain <- predict(model$finalModel,trainDat,predict.all=TRUE) 
sdpred <-  apply(predtrain$individual,1,sd)
```



# Standard deviation from individual trees for comparison

For camparison to what is often used as uncertainty information, the standard deviations of the individual predictions from the 500 developed trees within the Random Forest model are calculated.

```{r RFsd, echo=FALSE}
RFsd <- function(predictors,model){
  prep <- as.data.frame(predictors)
  prep[is.na(prep)] <- -9999
  pred_all <- predict(model$finalModel,prep,predict.all=TRUE)
  sds <-  apply(pred_all$individual,1,sd)
  predsd <- predictors[[1]]
  values(predsd) <- sds
  values(predsd)[prep[,1]==-9999] <- NA
  return(predsd)
}
```


```{r calcRFsd}
predsd <- RFsd(predictors,model)

```


# Comparison

The dissimilarity index, as well as the standard deviations can then be compared to the true error. 

```{r prepare_comp}
compare <- stack(response,prediction,
                 predsd,truediff, 
                 AOA$DI)
names(compare) <- c("response","prediction", "sd","true_diff","DI")
summary(values(compare))
```

```{r plot_comp, message=FALSE, echo=FALSE}
p <- list()
txts <- c("a","b","c","d","e","f")
for (i in 1:nlayers(compare)){
  cols <-viridis(100)
  txt <- list("sp.text", c(-12,72), txts[i])
  p[[i]] <- spplot(compare[[i]],col.regions = cols, 
                   sp.layout=list(txt))
}


p[[i+1]] <- sp::spplot(prediction, col.regions=viridis(100),sp.layout=list(list("sp.text", c(-12,72), txts[i+1])))+sp::spplot(AOA$AOA,col.regions=c("violetred","transparent"))


png("../figures/caseStudy_PredRef.png",width=24, height=14,units = "cm",res=300)
grid.arrange(p[[1]],p[[2]], p[[3]],
             p[[4]],p[[5]],p[[6]],
             nrow=2,ncol=3)
invisible(dev.off())
```

```{r figs4, echo=FALSE,out.width = "100%", fig.cap="Comparison between reference (a), prediction (b), standard deviation of predictions (c), the true error (d), DI based on weighted variables (e), masked predictions (f)"}


include_graphics("../figures/caseStudy_PredRef.png")
```




## Comparison between prediction, error, DI and cross validation
```{r lm_uncert}

#DI with weights:
cor(values(truediff),values(AOA$DI),use="complete.obs")
#DI no weights:
cor(values(truediff),values(AOA_noWeights$DI),use="complete.obs")

#comparison prediction~ref
summary(lm(values(response)~values(prediction)))$r.squared
cor(values(response),values(prediction),use = "complete.obs")

rmse(values(response),values(prediction))

#comparison prediction for the AOA~ref
print(attributes(AOA)$aoa_stats)
predictionAOI <- prediction
values(predictionAOI)[values(AOA$AOA)==0] <- NA
paste0("R^2 AOA= ",summary(lm(values(response)~values(predictionAOI)))$r.squared)
paste0("r AOA= ",cor(values(response),values(predictionAOI),use="complete.obs"))
paste0("RMSE AOA= ",rmse(values(response),values(predictionAOI)))

# ...and for outside the AOA
predictionNOTAOI <- prediction
values(predictionNOTAOI)[values(AOA$AOA)==1] <- NA
paste0("R^2 outside AOA= ",summary(lm(values(response)~values(predictionNOTAOI)))$r.squared)
paste0("r outside AOA= ",cor(values(response),values(predictionNOTAOI),use="complete.obs"))
paste0("RMSE outside AOA= ",rmse(values(response),values(predictionNOTAOI)))

```

## Standard deviations of predictions within the AOA
```{r lm_SDwithinAOA}
spplot(compare$sd,col.regions=viridis(100))+spplot(AOA$AOA,col.regions=c("violetred","transparent"))
```

## If applicable: Comparison random vs spatial CV and AOA estimation

#...if clustered compare to default AOA without taking
#clusteredness into account 
#should be comparable to random CV error
```{r lm_uncert_sp}
if(design=="clustered"){
  prediction_random <- predict(predictors,model_random)
  
predictionAOI <- prediction_random
values(predictionAOI)[values(AOA_random$AOA)==0] <- NA
print(paste0("R^2 AOA= ",summary(lm(values(response)~values(predictionAOI)))$r.squared))
print(paste0("r AOA= ",cor(values(response),values(predictionAOI),use="complete.obs")))

print(paste0("RMSE AOA= ", rmse(values(response),values(predictionAOI))))

predictionNOTAOI <- prediction_random
values(predictionNOTAOI)[values(AOA_random$AOA)==1] <- NA
print(paste0("R^2 outside AOA= ",summary(lm(values(response)~values(predictionNOTAOI)))$r.squared))
print(paste0("r outside AOA= ",cor(values(response),values(predictionNOTAOI),use="complete.obs")))
print(paste0("RMSE outside AOA= ",rmse(values(response),values(predictionNOTAOI))))
}
```

```{r compare_spatial,echo=FALSE}
if (design=="clustered"){
p <- list()
p[[1]] <-  spplot(AOA$DI,col.regions = cols,
            sp.layout=list(list("sp.points", samplepoints, col = "red", first = FALSE),list("sp.text", c(-12,72), "a")))
p[[2]] <- sp::spplot(prediction, col.regions=viridis(100),sp.layout=list(list("sp.text", c(-12,72), "b")))+sp::spplot(AOA$AOA,col.regions=c("violetred","transparent"))

p[[3]] <- sp::spplot(prediction_random, col.regions=viridis(100),sp.layout=list(list("sp.text", c(-12,72), "c")))+sp::spplot(AOA_random$AOA,col.regions=c("violetred","transparent"))

png("../figures/caseStudy_SPvsRandom.png",width=24, height=9,units = "cm",res=300)

grid.arrange(p[[1]],p[[2]], p[[3]],
             nrow=1,ncol=3)
invisible(dev.off())
}
```

```{r figs6, echo=FALSE, out.width="100%",fig.cap="Comparison between spatial vs non spatial estimation of the AOA"}
if(design=="clustered"){
  include_graphics("../figures/caseStudy_SPvsRandom.png")
}
```

## DI as a quantitative uncertainty measure?

### Relationship between the DI and the absolute error on a data-point-level

```{r hexbin, warning=FALSE, message=FALSE, echo=FALSE}
dat_all <- data.frame("true_abs_error"=values(truediff),                    "type"=rep("DI",length(values(AOA$DI))), 
                      "value"=values(AOA$DI))
th <- attributes(AOA)$aoa_stats$threshold

dat2 <- data.frame("DI"=attributes(AOA)$TrainDI,
                   "error"=absError)


png("../figures/hexbin.png", width=11, height=10,units="cm",res=300)
ggplot(dat_all, aes(value,true_abs_error)) +
  stat_binhex(bins=100)+
  ylab("True absolute error")+
  xlab("Dissimilarity Index")+
  geom_vline(xintercept=th,linetype="dashed",col="red")+
  scale_fill_gradientn(name = "data points",
                       trans = "log",
                       breaks = 10^(0:3),
                       colors=viridis(10, alpha=.7))+
  theme_bw()+
 theme(legend.title = element_text( size = 10))+
  geom_point(data=dat2,aes(DI,error))+
  stat_smooth(data=dat2,aes(DI,error),method="lm",se = FALSE,col="red")

invisible(dev.off())


```



```{r figs5, echo=FALSE, out.width="50%",fig.cap="Relationship between the DI and the true error"}
include_graphics("../figures/hexbin.png")
```


### Relationship between the DI and the requested performance measure (here RMSE) 



```{r userAOA_cv, warning=FALSE, message=FALSE, echo=FALSE,include=FALSE}


AOA_new <- calibrate_aoa(AOA,model,window.size = 10,showPlot=FALSE)


recl <- attributes(AOA_new)$calib$group_stats

reference_perf <- as.data.frame(stack(AOA$DI,response,prediction))
reference_perf <- reference_perf[order(reference_perf$DI),]
names(reference_perf) <- c("DI","obs","pred")


reference_perf$metric <- zoo::rollapply(reference_perf[,c("pred","obs")], width=1000,FUN=function(x){rmse(x[,1],x[,2])},
                    by.column=F,align = "center",fill=NA)
reference_perf <- reference_perf[reference_perf$DI<=max(recl$DI,na.rm=T),]

cairo_pdf("../figures/AOA_calibrate_singleCV.pdf", width=6, height=5)
plot(recl$DI,recl$RMSE,xlab="DI",ylab=model$metric)
points(reference_perf$DI,reference_perf$metric,col="red")
legend("topleft",lty=c(NA,NA,2),lwd=c(NA,NA,1),pch=c(1,NA,NA),col=c("black","red","black"),
       legend=c("CV","Truth","model"),bty="n")
  lines(seq(0,max(reference_perf$DI,na.rm=T),0.01),predict(attributes(AOA_new)$calib$model,data.frame("DI"=seq(0,max(reference_perf$DI,na.rm=T),0.01))),lwd=1,lty=2,col="black")
  
invisible(dev.off())



png("../figures/errormap_group_singleCV.png",width=24, height=9,units = "cm",res=300)
#grid.arrange(p[[1]],p[[2]], p[[3]],ncol=3)
stck <- stack(AOA_new$expectedError,mask(truediff,AOA_new$expectedError),
             mask(compare$sd,AOA_new$expectedError))
names(stck) <- c("expected_error","true_error","sd_of_predictions")
spplot(stck,col.regions=viridis(100))+
    sp::spplot(AOA_new$AOA,col.regions=c("violetred","transparent"))
invisible(dev.off())
```

```{r figs7, echo=FALSE, out.width="100%",fig.cap=""}
include_graphics("../figures/AOA_calibrate_singleCV.pdf")
```



And the same but  including re-fitting and validation with leave-cluster-in-predictorspace-out CV.

```{r userAOA_multicv, warning=FALSE, message=FALSE, echo=FALSE,include=FALSE}

AOA_new <- calibrate_aoa(AOA,model,multiCV=multiCV,length.out = 10,window.size = 25,showPlot=FALSE)


recl <- attributes(AOA_new)$calib$group_stats

reference_perf <- as.data.frame(stack(AOA$DI,response,prediction))
reference_perf <- reference_perf[order(reference_perf$DI),]
names(reference_perf) <- c("DI","obs","pred")

  reference_perf$metric <- zoo::rollapply(reference_perf[,c("pred","obs")], width=1000,FUN=function(x){rmse(x[,1],x[,2])},
                    by.column=F,align = "center",fill=NA)

  reference_perf <- reference_perf[reference_perf$DI<=max(recl$DI,na.rm=T),]
  


cairo_pdf("../figures/AOA_calibrate.pdf", width=6, height=5)
plot(recl$DI,recl$RMSE,xlab="DI",ylab=model$metric)
points(reference_perf$DI,reference_perf$metric,col="red")
legend("topleft",lty=c(NA,NA,2),lwd=c(NA,NA,1),pch=c(1,NA,NA),col=c("black","red","black"),
       legend=c("CV","Truth","model"),bty="n")
lines(seq(0,max(reference_perf$DI,na.rm=T),0.01),predict(attributes(AOA_new)$calib$model,data.frame("DI"=seq(0,max(reference_perf$DI,na.rm=T),0.01))),col="black",lty=2)
invisible(dev.off())


png("../figures/errormap_group.png",width=24, height=9,units = "cm",res=300)
stck <- stack(AOA_new$expectedError,mask(truediff,AOA_new$expectedError),
             mask(compare$sd,AOA_new$expectedError))
names(stck) <- c("expected_error","true_error","sd_of_predictions")
spplot(stck,col.regions=viridis(100))+
    sp::spplot(AOA_new$AOA,col.regions=c("violetred","transparent"))
invisible(dev.off())
```


```{r figs8, echo=FALSE, out.width="100%",fig.cap=""}
include_graphics("../figures/AOA_calibrate.pdf")
```


```{r figs9, echo=FALSE, out.width="100%",fig.cap=""}
include_graphics("../figures/errormap_group.png")
```




